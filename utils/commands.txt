(rtx8000) 
srun --pty -p gpu-long --mem=32000 --partition=gypsum-rtx8000 --gres=gpu:1 bash
srun --pty -p gpu-long --mem=32000 --constraint="linux-ubuntu20.04-skylake_avx512" --gres=gpu:1 --time=14-00:00:00 bash
(v100)
srun --pty -p gpu-long --mem=32000 --constraint="v100" --gres=gpu:1 --time=14-00:00:00 bash
srun --pty -p gpu-long --mem=32000 --partition=gypsum-2080ti --gres=gpu:1 bash
srun --pty -p gpu-long --mem=32000 --partition=gypsum-m40 --gres=gpu:2 bash
srun --pty -p gpu-long --mem=32000 --partition=gypsum-titanx --gres=gpu:1 bash
(a100 preempt)
srun --pty -p gpu-preempt --mem=32000 --constraint="a100" --gres=gpu:1 --time=14-00:00:00 bash


(a100) 
module load python/3.9.1 && module load cuda/11.0.1 && source /work/nigel_umass_edu/env-a100/bin/activate && cd /work/nigel_umass_edu/code/qg_challenge
(old) module load python/3.9.1 && module load cuda/11.0.1 && source /work/nigel_umass_edu/myenv/bin/activate && cd /work/nigel_umass_edu/code/qg_challenge
(old) module load python/3.9.1 && module load cuda/11.0.1 && source /work/nigel_umass_edu/myenv/bin/activate && cd /home/nigel_umass_edu/qg_challenge


Finetune full model name: curie:ft-umass-amherst:curie-train-2022-11-03-00-04-39
Finetune small model name: curie:ft-umass-amherst:curie-train-small-2022-11-02-18-12-00


-> Local validation set:

python -m code.private.gpt3.eval_local \
    --model_name "curie:ft-umass-amherst:curie-train-2022-11-03-00-04-39" \
    --eval_type "local_val" 


Zero shot latest curie model:
python -m code.gpt3.evaluate \
    --model_name "text-curie-001" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" 


Zero shot latest davinci model:
python -m code.private.gpt3.eval_local \
    --model_name "text-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?"




->Public test leaderboad evaluation

Finetuned curie model:
python -m code.private.gpt3.eval_local \
    --model_name "curie:ft-umass-amherst:curie-train-2022-11-03-00-04-39" \
    --eval_type "leaderboard_public_test" \
    --eval_folder "original" \
    --eval_filename "test.csv" 






Finetune GPT-2 XL model with CLM loss only on question tokens:
python3.9 -m code.gpt2.train \
    --name "gpt2-xl" \
    --lm gpt2-xl \
    --data_folder "train_val_split_json" \
    --batch_size 4 \
    --lm_loss_location "question" \
    --log_wandb 


Finetune GPT-2 XL model with CLM loss on all tokens:
python3.9 -m code.gpt2.train \
    --name "gpt2-xl" \
    --lm gpt2-xl \
    --data_folder "train_val_split_json" \
    --batch_size 4 \
    --lm_loss_location "all" \
    --log_wandb 


Debug:
python3.9 -m code.gpt2.train \
    --name "gpt2-trial" \
    --lm gpt2 \
    --data_folder "train_val_split_json" \
    --batch_size 4 \
    --lm_loss_location "question" \
    --log_wandb \
    --debug --iters 2 

Debug:
python3.9 -m code.gpt2.train \
    --name "gpt2-trial" \
    --lm gpt2 \
    --data_folder "train_val_split_json" \
    --batch_size 4 \
    --lm_loss_location "all" \
    --log_wandb \
    --debug --iters 2 



GPT-2 BLEURT eval:

-> gpt2-xl/different-donkey-103 is all tokens
CLM loss on all tokens

Beam search:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/different-donkey-103" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "beam_search" \
    --add_instructions \
    --cuda 

Greedy:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/different-donkey-103" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "greedy" \
    --add_instructions \
    --cuda 

top_k_sampling:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/different-donkey-103" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "top_k_sampling" \
    --add_instructions \
    --cuda 

nucleus_sampling:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/different-donkey-103" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "nucleus_sampling" \
    --add_instructions \
    --cuda 

nucleus_sampling_with_top_k:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/different-donkey-103" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "nucleus_sampling_with_top_k" \
    --add_instructions \
    --cuda 


-> gpt2-xl/crisp-oath-102 is question only
CLM loss on question only


Beam search:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/crisp-oath-102" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "beam_search" \
    --add_instructions \
    --cuda

Greedy:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/crisp-oath-102" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "greedy" \
    --add_instructions \
    --cuda 

top_k_sampling:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/crisp-oath-102" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "top_k_sampling" \
    --add_instructions \
    --cuda 

nucleus_sampling:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/crisp-oath-102" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "nucleus_sampling" \
    --add_instructions \
    --cuda 

nucleus_sampling_with_top_k:
python3.9 -m code.gpt2.evaluate \
    --finetuned \
    --model_folder "gpt2-xl/crisp-oath-102" \
    --eval_type "local_val" \
    --eval_folder "train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "nucleus_sampling_with_top_k" \
    --add_instructions \
    --cuda 



In-context prompting:




python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "all_types"


python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "all_types" \
    --pack_max 


python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "random" 


python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "random" \
    --pack_max 


python -m code.gpt3.evaluate \
    --model_name "text-curie-001" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "all_types"


python -m code.gpt3.evaluate \
    --model_name "text-curie-001" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "random"


Zero shot codex model:
python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?"


Zero shot gpt3 davinci model:

python -m code.gpt3.evaluate \
    --model_name "text-ada-001" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?"


Zero shot new gpt3 davinci model:

python -m code.gpt3.evaluate \
    --model_name "text-davinci-003" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?"


Incontext prompting

Retrieval:
python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_all_types" \
    --retrieval_query "story_answer" 
code-davinci-002_20221124-033430
0.48661857419382265

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_all_types" \
    --retrieval_query "story" 

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_all_types" \
    --retrieval_query "answer" 

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_topk" \
    --retrieval_query "story_answer" 
code-davinci-002_20221124-035154
0.4885043883544764

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_topk" \
    --retrieval_query "story" 

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "retrieval_topk" \
    --retrieval_query "answer" 



Augment:
python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "augment"
code-davinci-002_20221123-204720
BLEURT = 0.5100783299185275

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "augment" \
    --augment_on_single_story
code-davinci-002_20221123-215506
BLEURT = 0.44696097889142794



Top-10 decoding:

python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --n 10 \
    --best_of 10 \
    --temperature 0.99 


python -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "leaderboard_public_test" \
    --eval_folder "original" \
    --eval_filename "test.csv" 
    --add_instructions \
    --stop "?" \
    --n 3 \
    --best_of 3 \
    --debug


python3.9 -m code.gpt3.evaluate \
    --model_name "code-davinci-002" \
    --eval_type "local_val" \
    --add_instructions \
    --stop "?" \
    --incontext \
    --incontext_mode "augment" \
    --n 10 \
    --best_of 10 \
    --temperature 0.7 



Data augmentation - old commands:

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 0 \
    --end_row 1000 

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 1000 \
    --end_row 2000

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 2000 \
    --end_row 3000

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 3000 \
    --end_row 4000

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 4000 \
    --end_row 5000

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --start_row 5000 \
    --end_row 6005



Data augmentation with exception handling, api key rotation, and smart time delay (no need to split):

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]"



Data augmentation with question attributes generated:

python3.9 -m code.dataaugmentation.augment \
    --model_name "code-davinci-002" \
    --stop "[End]" \
    --question_attribute_before \
    --question_attributes_desc


srun --pty -p gpu-long --mem=32000 --constraint="v100" --gres=gpu:1 --time=14-00:00:00 bash



-----
->Flan-T5 finetuning:

qg challenge data: S+A -> Q

(local)
python3.9 -m code.t5.train \
    --lm google/flan-t5-small \
    --cross_val_fold 21 \
    --debug --iters 1 \
    --neptune

(server)
python3.9 -m code.t5.train \
    --lm google/flan-t5-large \
    --cross_val_fold 21 \
    --neptune --cuda \
    --debug --iters 2



evaluate:
(local)
python -m code.t5.evaluate \
    --model_folder "qg_challenge/QGCHAL-60/cross_val_fold_21/best_val_score" \
    --eval_type "local_val" \
    --eval_folder "folds/seed_21/train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "contrastive_search" \
    --debug \
    --num_return_sequences 10

(server)
python -m code.t5.evaluate \
    --model_folder "qg_challenge/QGCHAL-79/cross_val_fold_21/best_val_score/" \
    --eval_type "local_val" \
    --eval_folder "folds/seed_21/train_val_split_csv" \
    --eval_filename "val.csv" \
    --decoding_type "contrastive_search" \
    --num_return_sequences 10 \
    --cuda



external data then qg challenge data: S+A -> Q

(external data augment first)
(local)
python3.9 -m code.t5.train \
    --lm google/flan-t5-small \
    --cross_val_fold 21 \
    --augment \
    --debug --iters 1 \
    --neptune


(server)
python3.9 -m code.t5.train \
    --lm google/flan-t5-large \
    --cross_val_fold 21 \
    --augment \
    --neptune --cuda \
    --batch_size_eval 64 \
    --batch_size 4


(then continue from checkpoint on S+A -> Q)
(local)
python3.9 -m code.t5.train \
    --lm google/flan-t5-small \
    --checkpoint \
    --model_folder qg_challenge/QGCHAL-60/cross_val_fold_21/best_val_score \
    --cross_val_fold 21 \
    --debug --iters 1 \
    --neptune

(server)
(flan-t5-large)
python3.9 -m code.t5.train \
    --lm google/flan-t5-large \
    --checkpoint \
    --model_folder qg_challenge/QGCHAL-63/cross_val_fold_21/best_val_score \
    --cross_val_fold 21 \
    --neptune --cuda

(flan-t5-base)
python3.9 -m code.t5.train \
    --lm google/flan-t5-base \
    --checkpoint \
    --model_folder qg_challenge/QGCHAL-64/cross_val_fold_21/best_val_score \
    --cross_val_fold 21 \
    --neptune --cuda





-----
->Overgenerate and rank:


(local)
((flan-t5-small)
python3.9 -m code.score_prediction.train \
    --lm google/flan-t5-small \
    --debug --iters 10 \
    --neptune

(bert)
python3.9 -m code.score_prediction.train \
    --lm bert-base-uncased \
    --debug --iters 10 \
    --neptune


(server)
(flan-t5-large with iters 5)
python3.9 -m code.score_prediction.train \
    --lm google/flan-t5-large \
    --iters 5 \
    --neptune --cuda

(flan-t5-large with iters 10)
python3.9 -m code.score_prediction.train \
    --lm google/flan-t5-large \
    --neptune --cuda

(flan-t5-base)
python3.9 -m code.score_prediction.train \
    --lm google/flan-t5-base \
    --neptune --cuda

(bert-base)
python3.9 -m code.score_prediction.train \
    --lm bert-base-uncased \
    --neptune --cuda 

(bert-large)
python3.9 -m code.score_prediction.train \
    --lm bert-large-uncased \
    --neptune --cuda