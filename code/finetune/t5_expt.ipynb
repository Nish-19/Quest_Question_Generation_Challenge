{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two training examples\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue Ã  NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input encoding\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  1566,    12,  2379,    10,  5242,    12, 13465,     1,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [13959,  1566,    12,  2379,    10, 11560,  3896,   371,  3302,    19,\n",
      "             3,     9,   349,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "labels = target_encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10520, 15098,     3,    85, 13465,     1,     0,     0],\n",
      "        [11560,  3896,   371,  3302,   259,   245, 11089,     1]])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id) # pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18801377713680267"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def get_parallel_corpus(ip_df, story_df):\n",
    "    # hash stories and sections\n",
    "    story_sec_hash = defaultdict(dict)\n",
    "    for i, row in story_df.iterrows():\n",
    "        story_sec_hash[row['source_title']][row['cor_section']] = row['text']\n",
    "    \n",
    "    story, answer, question = [], [], []\n",
    "    for i, row in ip_df.iterrows():\n",
    "        sec_nums = row['cor_section'].split(',')\n",
    "        story_str = ''\n",
    "        for sec_num in sec_nums:\n",
    "            story_str += story_sec_hash[row['source_title']][int(sec_num)]\n",
    "        story.append(story_str)\n",
    "        answer.append(row['answer'])\n",
    "        question.append(row['question'])\n",
    "    \n",
    "    return story, answer, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_file = '../../data/original/source_texts.csv'\n",
    "story_df = pd.read_csv(story_file)\n",
    "# Train-Val split\n",
    "train_file = '../../data/train_val_split_csv/train.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "val_file = '../../data/train_val_split_csv/val.csv'\n",
    "val_df = pd.read_csv(val_file)\n",
    "\n",
    "train_story, train_answer, train_question = get_parallel_corpus(train_df, story_df)\n",
    "val_story, val_answer, val_question = get_parallel_corpus(val_df, story_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(story, answer, question):\n",
    "    print('Average story length:', statistics.mean([len(stry) for stry in story]))\n",
    "    print('Average answer length:', statistics.mean([len(ans) for ans in answer]))\n",
    "    print('Average question length:', statistics.mean([len(quest) for quest in question]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Average story length: 1015.8474604496253\n",
      "Average answer length: 36.77901748542881\n",
      "Average question length: 51.96319733555371\n",
      "Valid Set\n",
      "Average story length: 987.5924796747968\n",
      "Average answer length: 33.672764227642276\n",
      "Average question length: 48.9369918699187\n"
     ]
    }
   ],
   "source": [
    "# print stats\n",
    "print('Train Set')\n",
    "get_stats(train_story, train_answer, train_question)\n",
    "\n",
    "print('Valid Set')\n",
    "get_stats(val_story, val_answer, val_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385658c852a81de54eda8e4f40dcebc3a4ec9c18780df63d8dc50503a2c10b12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
