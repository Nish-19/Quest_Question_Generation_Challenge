{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two training examples\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue Ã  NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input encoding\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  1566,    12,  2379,    10,  5242,    12, 13465,     1,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [13959,  1566,    12,  2379,    10, 11560,  3896,   371,  3302,    19,\n",
      "             3,     9,   349,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "labels = target_encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10520, 15098,     3,    85, 13465,     1,     0,     0],\n",
      "        [11560,  3896,   371,  3302,   259,   245, 11089,     1]])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id) # pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18801377713680267"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def get_parallel_corpus(ip_df, story_df):\n",
    "    # hash stories and sections\n",
    "    story_sec_hash = defaultdict(dict)\n",
    "    for i, row in story_df.iterrows():\n",
    "        story_sec_hash[row['source_title']][row['cor_section']] = row['text']\n",
    "    \n",
    "    story, answer, question = [], [], []\n",
    "    for i, row in ip_df.iterrows():\n",
    "        sec_nums = row['cor_section'].split(',')\n",
    "        story_str = ''\n",
    "        for sec_num in sec_nums:\n",
    "            story_str += story_sec_hash[row['source_title']][int(sec_num)]\n",
    "        story.append(story_str)\n",
    "        answer.append(row['answer'])\n",
    "        question.append(row['question'])\n",
    "    \n",
    "    return story, answer, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_file = '../../data/original/source_texts.csv'\n",
    "story_df = pd.read_csv(story_file)\n",
    "# Train-Val split\n",
    "train_file = '../../data/train_val_split_csv/train.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "val_file = '../../data/train_val_split_csv/val.csv'\n",
    "val_df = pd.read_csv(val_file)\n",
    "\n",
    "train_story, train_answer, train_question = get_parallel_corpus(train_df, story_df)\n",
    "val_story, val_answer, val_question = get_parallel_corpus(val_df, story_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(story, answer, question):\n",
    "    print('Average story length:', statistics.mean([len(stry) for stry in story]))\n",
    "    print('Average answer length:', statistics.mean([len(ans) for ans in answer]))\n",
    "    print('Average question length:', statistics.mean([len(quest) for quest in question]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Average story length: 1015.8474604496253\n",
      "Average answer length: 36.77901748542881\n",
      "Average question length: 51.96319733555371\n",
      "Valid Set\n",
      "Average story length: 987.5924796747968\n",
      "Average answer length: 33.672764227642276\n",
      "Average question length: 48.9369918699187\n"
     ]
    }
   ],
   "source": [
    "# print stats\n",
    "print('Train Set')\n",
    "get_stats(train_story, train_answer, train_question)\n",
    "\n",
    "print('Valid Set')\n",
    "get_stats(val_story, val_answer, val_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrcut t5 input \n",
    "def construct_t5_input(story, answer):\n",
    "    inps = []\n",
    "    prefix = 'Generate question from story and answer: '\n",
    "    for stry, ans in zip(story, answer):\n",
    "        t5_input = prefix + ' The story is ' + stry + ' The answer is ' + ans \n",
    "        inps.append(t5_input)\n",
    "    return inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inps = construct_t5_input(train_story, train_answer)\n",
    "val_inps = construct_t5_input(val_story, val_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t5_encoding(t5_inputs, answer):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    max_source_length, max_target_length = 512, 128\n",
    "\n",
    "    inp_encoding = tokenizer(t5_inputs, padding='longest', \n",
    "                        max_length=max_source_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "    input_ids, attention_mask = inp_encoding.input_ids, inp_encoding.attention_mask\n",
    "\n",
    "    target_encoding = tokenizer(answer, padding='longest', \n",
    "                        max_length=max_target_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "    \n",
    "    labels = target_encoding.input_ids\n",
    "\n",
    "    # 0 loss for pad tokens\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_mask, train_labels = get_t5_encoding(train_inps, train_answer)\n",
    "val_input_ids, val_attention_mask, val_labels = get_t5_encoding(val_inps, val_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairyDataset(Dataset):\n",
    "    def __init__(self, input_ids, attn_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_masks = attn_masks\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.input_ids[index]\n",
    "        y = self.attn_masks[index]\n",
    "        z = self.labels[index]\n",
    "        \n",
    "        return {'input_ids': x, 'attention_mask': y, 'labels':z}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FairyDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = FairyDataset(val_input_ids, val_attention_mask, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, dataset):\n",
    "    return DataLoader(dataset=dataset, shuffle=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = get_dataloader(batch_size, train_dataset)\n",
    "valid_dataloader = get_dataloader(batch_size, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Model with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneT5(pl.LightningModule):\n",
    "    def __init__(self, lr=5e-5, num_train_epochs=5, warmup_steps=1000):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):     \n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs\n",
    "    \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # create optimizer\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        # create learning rate scheduler\n",
    "        num_train_optimization_steps = self.hparams.num_train_epochs * len(train_dataloader)\n",
    "        lr_scheduler = {'scheduler': get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                                                    num_training_steps=num_train_optimization_steps),\n",
    "                        'name': 'learning_rate',\n",
    "                        'interval':'step',\n",
    "                        'frequency': 1}\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb, os\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'FinetuneT5'\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuneT5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainig code\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "wandb_logger = WandbLogger(name='FinetuneT5', project='Quest_Gen_Challenge')\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='validation_loss',\n",
    "    patience=3,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "\n",
    "trainer = Trainer(gpus=0, \n",
    "                  default_root_dir=\"./Checkpoints\", \n",
    "                  logger=wandb_logger, \n",
    "                  callbacks=[early_stop_callback, lr_monitor])\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './Checkpoints'\n",
    "model.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385658c852a81de54eda8e4f40dcebc3a4ec9c18780df63d8dc50503a2c10b12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
